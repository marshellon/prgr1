{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy\n",
    "\n",
    "NumPy (http://numpy.org) is a module for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.  The name is an acronym for \"Numeric Python\" or \"Numerical Python\". It is an extension module for Python, mostly written in C. This makes sure that the precompiled mathematical and numerical functions and functionalities of Numpy guarantee great execution speed. \n",
    "\n",
    "NumPy enriches the programming language Python with powerful data structures, implementing multi-dimensional arrays and matrices. These data structures guarantee efficient calculations with matrices and arrays. The implementation is even aiming at huge matrices and arrays, better know under the heading of \"big data\". Besides that the module supplies a large library of high-level mathematical functions to operate on these matrices and arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study case the linear regression model error\n",
    "In the example below the usage of NumPy and the matrix calculations are demonstrated. In the example the cost $J(\\theta)$ (the error) of a linear regression model is computed using the equation:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} ( h_\\theta(x^{(i)}) - y^{(i)} ) ^2$$\n",
    "\n",
    "\n",
    "where \n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3 + ..\\theta_nx_n$$\n",
    "\n",
    "In which $J(\\theta)$ is the total cost calculated by the current weight values of $\\theta$; $h_\\theta(x)$ is the hypothesed value, the prediction, and $y$ is the actual value. $h_\\theta(x)$ is calculated for each observation $h_\\theta(x^{(i)})$ and compared to the actual value $y^{(i)}$. By adding up and eventually averaging the difference between these two values (hypotesis - actual) for each data observation, we arrive at the predictive value that the formula has with the current weight values of $\\theta$.\n",
    "\n",
    "To compute this we can use a naive loop or we can use the matrix computation functions included in Numpy, the so called vectorized implementation. To demonstrate the difference in performance solutions for both methods are provided. We time the execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use NumPy to generate $m \\times n$ matrix and $m \\times 1$ vector and $1 \\times n$ vector\n",
    "First a dataset is generated. The dataset contains a number of features (columns in the dataset except the last one $n -1$). The final column contains a class variabele. The dataset has a number of observations (the rows $m$). For this the numpy function `np.random.rand(m, n)` is used. Next a vector containing the weights is generated (the $\\theta$ vector). The last column, containing the class variable is sliced to a vector $y$ and the features columns are put into a matrix $X$. For computational purpose a column of 1's is added to the feature matrix $X$\n",
    "\n",
    "\\begin{equation}\n",
    "X = \n",
    "   \\begin{bmatrix} \\\n",
    "   1 & x_1^{(1)}  & x_2^{(1)} & .. & x_n^{(1)}\\\\\n",
    "   1 & x_1^{(2)}  & x_2^{(2)} & .. & x_n^{(2)}\\\\ \n",
    "   1 & x_1^{(3)}  & x_2^{(3)} & .. & x_n^{(3)} \\\\ \n",
    "   1 & .. & .. & .. & .. \\\\ \n",
    "   1 & x_1^{(m)}  & x_2^{(m)} & .. & x_n^{(m)} \\\\ \n",
    "   \\end{bmatrix} \n",
    "   \\\n",
    "   %\n",
    "   y = \n",
    "   \\begin{bmatrix} \\\n",
    "   y^{(1)} \\\\\n",
    "   y^{(2)} \\\\ \n",
    "   y^{(3)} \\\\ \n",
    "   .. \\\\ \n",
    "   y^{(m)} \\\\ \n",
    "   \\end{bmatrix} \n",
    "   %\n",
    "   \\\n",
    "   \\theta = \n",
    "   \\begin{bmatrix}\n",
    "    \\theta_0 & \\theta_1 & .. & \\theta_n \n",
    "  \\end{bmatrix}\n",
    "  %\n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y (50000, 1) vector\n",
      "X (50000, 150) matrix\n",
      "𝜃 (1, 150) vector\n",
      "There are 150 features,  and 50000 observations\n"
     ]
    }
   ],
   "source": [
    "num_features = 150\n",
    "num_observaties = 50000\n",
    "data = np.random.rand(num_observaties, num_features) #generate dataset\n",
    "theta = np.random.rand(1,num_features) #generate vector containing weights\n",
    "\n",
    "m,n = data.shape\n",
    "X = data[:, :n-1]    #all the columns except the last one contain the features\n",
    "y = data[:, [n-1]]   #last column is the class variable\n",
    "X = np.c_[np.ones(m), X] #add a first column with ones for the theta0 compution\n",
    "\n",
    "print(\"y\", y.shape, \"vector\")\n",
    "print(\"X\", X.shape, \"matrix\")\n",
    "print(\"𝜃\", theta.shape, \"vector\")\n",
    "print (f\"There are {num_features} features,  and {num_observaties} observations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive loop implementation\n",
    "\n",
    "The naive loop implementation of calculating the error $J$ computes for each row $i$ the prediction $h$ which is substracted with the actual value ($h - y_i$) to get the difference between the actual value and the model value. The prediction is calculated using a for loop to compute the weight times the feature value for each feature according the equation $ h = \\theta_0 \\times 1 + \\theta_1 \\times x_1 + \\theta_2 \\times x_2 + ...\\theta_n \\times x_n$. The difference between the actual value and the model value is squared and averaged to estimate the average error of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive implementation\n",
      "Error: [636.06025802]\n",
      "Execution time 5116 millis\n"
     ]
    }
   ],
   "source": [
    "#naive implementation\n",
    "print (\"Naive implementation\")\n",
    "start_time = int(round(time.time() * 1000))\n",
    "J_val1 = 0\n",
    "theta_nav = theta[0] # get rid of the [[]] -> []\n",
    "for i in range(m):\n",
    "    xi = X[i]\n",
    "    prediction = 0\n",
    "    for j in range(len(theta_nav)):\n",
    "        prediction += theta_nav[j]*xi[j] # predict value based on weight theta and feature xi\n",
    "    delta = (prediction - y[i]) ** 2     # square difference of hypothesed value and actual value\n",
    "    J_val1 += delta                      # sum of squares\n",
    "\n",
    "J_val_nav = J_val1/ (2 * m)              # take average of sum of squares\n",
    "end_time = int(round(time.time() * 1000))\n",
    "print (f\"Error: {J_val_nav}\")\n",
    "print (f\"Execution time {end_time - start_time} millis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized implemention\n",
    "\n",
    "For the hypothesis we can use a vectorized implementation: $ h_\\theta(x) = \\theta^T.X $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorial implementation\n",
      "Error: 636.060258018252\n",
      "Execution time 4 millis\n"
     ]
    }
   ],
   "source": [
    "#vectorial implementatie\n",
    "print (\"Vectorial implementation\")\n",
    "\n",
    "start_time = int(round(time.time() * 1000))\n",
    "h = np.dot(X, theta.T)            #matrix calculation features times weights theta resulting in prediction vector\n",
    "errors = (h - y) ** 2             #vector substraction predictions minus actual values\n",
    "J_val_vec = np.mean(errors)/2     #vector average\n",
    "\n",
    "end_time = int(round(time.time() * 1000))\n",
    "print (f\"Error: {J_val_vec}\")\n",
    "print (f\"Execution time {end_time - start_time} millis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Conclusion\n",
    "\n",
    "- With Numpy we can easilly generate and manipulate vectors and matrices. \n",
    "- We can transpose vectors and matrices using .T\n",
    "- We can apply vectorized computations using power, division, substractions, mutiplications with `np.dot` and get mean with `np.mean`\n",
    "- Vectorized implementation is incredibly faster than an ordinary loop\n",
    "- You should use Numpy arrays, or a library that build upon Numpy like pandas, for data processing\n",
    "- **You are stupid if you use a for loop for dataprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "\n",
    "Learn more about Numpy: https://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/tools_numpy.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
