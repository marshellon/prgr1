{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files and formats\n",
    "\n",
    "So far we worked with flat files and csv files. Today we talk a bit more about JSON, and Web API's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has a number of methods for reading tabular data as a DataFrame object. \n",
    "\n",
    "        read_csv\n",
    "        read_fwf\n",
    "        read_clipboard\n",
    "        read_excel\n",
    "        read_hdf\n",
    "        read_html\n",
    "        read_json\n",
    "        read_msgpack\n",
    "        read_pickle\n",
    "        read_sas\n",
    "        read_sql\n",
    "        read_stata\n",
    "        read_feather\n",
    "        \n",
    "So far we used most of the time `read_csv`, `read_fwf`, `read_excel`. We experienced that data can be very messy and that we need a lot of different parameters to perform a nice read without errors. Sometimes the data is seperated by variable amount of whitespaces. In these cases we can pass a regular expression as a delimiter for read_csv. \n",
    "\n",
    "        data = pd.read_csv('example.txt', sep = '\\s+')\n",
    "        \n",
    "Other handy methods are\n",
    "\n",
    "        na_values\n",
    "        skiprows\n",
    "        sep\n",
    "        nrows\n",
    "        chunksize\n",
    "        skip_footer\n",
    "        encoding\n",
    "        \n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Data\n",
    "\n",
    "https://www.youtube.com/watch?v=EfEm0g-bMPc\n",
    "\n",
    "JSON has become one of the standard for sending http requests between webbrowsers and other applications. JSON is very nearly valid Python code using basic types like dictionaries, arrays, strings, numbers and booleans. So far we used `json.loads()` and `json.dumps()` to read and write json files. JSON data can be converted to a dataframe using the built in pandas `pd.read_json` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "db = json.load(open('data/food.json'))\n",
    "print(db[0:1])\n",
    "#each entry is a dict, so we select one of these\n",
    "db[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('data/food.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The disadvantage of `pd.read_json` is that we create columns with dictionaries. Better is to select the nutrients information from the column and put it in a different dataframe along with the `id` number, so that we can combine the two afterwards. Below you see how the first row and column cell is selected and put in a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nutrients = pd.DataFrame(db[0]['nutrients'])\n",
    "nutrients.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this for the entire dataframe we need to loop through the dataframe and put each nutrients cell in a dataframe, add the `id` column for identification purpose. If we first create a list of all these dataframes and then concat them we have one single dataframe for all the nutrients cells in the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrients = []\n",
    "for rec in db:\n",
    "    fnuts = pd.DataFrame(rec['nutrients'])\n",
    "    fnuts['id'] = rec['id']\n",
    "    nutrients.append(fnuts)\n",
    "\n",
    "nutrients = pd.concat(nutrients, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nutrients[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicates\n",
    "print(len(nutrients))\n",
    "nutrients.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrients = nutrients.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(nutrients))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we put the nutrients info in a seperate dataframe we can eliminate that from the original database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['description',\n",
    "                   'group',\n",
    "                   'id',\n",
    "                   'manufacturer']\n",
    "df = df[columns_to_keep]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we reduced the dimensions in the dataframes we can easily merge them. First we rename the columns that are in both dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {'description':'food', 'group':'food_group'}, copy = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrients = nutrients.rename(columns = {'description':'nutrients', 'group': 'nutrient_group'})\n",
    "nutrients.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata = pd.merge(nutrients, df, on='id', how='outer')\n",
    "ndata.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the merged data we can conduct any analysis we like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "result = ndata.groupby(['nutrients', 'food_group'])['value'].quantile(0.5)\n",
    "result['Total lipid (fat)'].sort_values().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving data troughout API's\n",
    "\n",
    "API stands for Application Programming Interface. It is the interface that allows software applications to communicate with one another. An API is a software-to-software interface, not a user interface. With APIs, applications talk to each other without any user knowledge or intervention.\n",
    "\n",
    "\n",
    "An example is the Twitter API. It is a web-based JSON API that allows developers to programmatically interact with Twitter data. The Twitter API is a web-based API. It must be accessed by making requests over the Internet to services that Twitter hosts. With a web-based API such as Twitter’s, your application sends an HTTP request, just like a web browser does. But instead of the response being delivered as a webpage, for human understanding, it’s returned in a format that applications can easily parse. Various formats exist for this purpose, and Twitter uses a popular and easy-to-use format called JSON. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to access Twitter Streaming API, we need to get 4 pieces of information from Twitter: API key, API secret, Access token and Access token secret.  If you to https://apps.twitter.com/ and log in with your twitter credentials you can create a New App and get the API key credentials for yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the twitter API we need the tweepy library see https://tweepy.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: http://adilmoujahid.com/posts/2014/07/twitter-analytics/\n",
    "#Import the necessary methods from tweepy library\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "#Variables that contains the user credentials to access Twitter API \n",
    "access_token = token\n",
    "access_token_secret = secret_token\n",
    "consumer_key = api_key\n",
    "consumer_secret = api_secret_key\n",
    "\n",
    "\n",
    "#This is a basic listener that just stores tweets in json file\n",
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "#        with open('data/out.json', 'a') as f:\n",
    "#            f.write(data)\n",
    "        print(data)\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by the keywords\n",
    "    stream.filter(track=['aerobic', 'anaerobic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a JSON format we can process the data accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "tweets_data_path = 'data/result2.json'\n",
    "\n",
    "tweets_data = []\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "for line in tweets_file:\n",
    "    tweet = json.loads(line)\n",
    "    tweets_data.append(tweet)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tweets_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweets = pd.DataFrame(tweets_data)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some useful regex methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_link(text):\n",
    "    import re\n",
    "    regex = r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+'\n",
    "    match = re.search(regex, text)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['link'] = tweets['text'].apply(lambda tweet: extract_link(tweet))\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_in_text(word, text):\n",
    "    import re\n",
    "    word = word.lower()\n",
    "    text = text.lower()\n",
    "    match = re.search(word, text)\n",
    "    if match:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['sport'] = tweets['text'].apply(lambda tweet: word_in_text('sport', tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Try to read the sql file in the data directory. More information is to be found here https://github.com/fenna/twitter_analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
